–û—Ç–ª–∏—á–Ω–æ! –°–µ–π—á–∞—Å –¥–µ—Ç–∞–ª—å–Ω–æ —Ä–∞—Å–ø–∏—à—É –∫–∞–∂–¥—É—é —Ñ–∏—á—É —Å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è.

---

# üìã –î–ï–¢–ê–õ–¨–ù–ê–Ø –†–ê–ó–ë–ò–í–ö–ê –ò–ù–ù–û–í–ê–¶–ò–ô

## **1. Sentiment Drift Detector (–î–µ—Ç–µ–∫—Ç–æ—Ä –∏–∑–º–µ–Ω–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è)**

### üéØ –ü—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏:
*–ü—Ä–µ–¥—Å—Ç–∞–≤—å, —á—Ç–æ —Ç—ã —á–∏—Ç–∞–µ—à—å –Ω–æ–≤–æ—Å—Ç–∏ –æ –∫–æ–º–ø–∞–Ω–∏–∏. –£—Ç—Ä–æ–º –ø–∏—à—É—Ç "–∫–æ–º–ø–∞–Ω–∏—è –æ–±—Å—É–∂–¥–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏", –≤ –æ–±–µ–¥ "–≤–æ–∑–º–æ–∂–Ω—ã –ø—Ä–æ–±–ª–µ–º—ã", –≤–µ—á–µ—Ä–æ–º "–∫–æ–º–ø–∞–Ω–∏—è –Ω–∞ –≥—Ä–∞–Ω–∏ –∫—Ä–∞—Ö–∞". –í–æ—Ç —ç—Ç–æ **–∏–∑–º–µ–Ω–µ–Ω–∏–µ** –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è –æ—Ç –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–≥–æ –∫ –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ–º—É –∏ –µ—Å—Ç—å —Å–∏–≥–Ω–∞–ª –æ–ø–∞—Å–Ω–æ—Å—Ç–∏. –ú—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º, –∫–∞–∫ –º–µ–Ω—è–µ—Ç—Å—è —Ç–æ–Ω –Ω–æ–≤–æ—Å—Ç–µ–π –≤–æ –≤—Ä–µ–º–µ–Ω–∏.*

### üì¶ –ß—Ç–æ –Ω—É–∂–Ω–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å:

**–î–∞–Ω–Ω—ã–µ:**
- ‚úÖ –£ –≤–∞—Å —É–∂–µ –µ—Å—Ç—å: —Å—Ç–∞—Ç—å–∏ —Å `published_at` –∏ `content`
- ‚ùå –ù—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å: sentiment –º–µ—Ç–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è

**–ú–æ–¥–µ–ª–∏:**
```python
# 1. Sentiment Analysis –º–æ–¥–µ–ª—å
- –í–∞—Ä–∏–∞–Ω—Ç A (–ø—Ä–æ—Å—Ç–æ–π): RuSentiment –∏–ª–∏ dostoevsky (–≥–æ—Ç–æ–≤—ã–µ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ)
- –í–∞—Ä–∏–∞–Ω—Ç B (—Ç–æ—á–Ω—ã–π): Fine-tuned BERT –Ω–∞ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç—è—Ö
- –†–∞–∑–º–µ—Ä: ~500MB –¥–ª—è BERT, ~100MB –¥–ª—è –ª–µ–≥–∫–æ–π –º–æ–¥–µ–ª–∏

# 2. Drift –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ (–±–µ–∑ ML, –ø—Ä–æ—Å—Ç–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞)
- Moving average sentiment
- Standard deviation
- Z-score –¥–ª—è –∞–Ω–æ–º–∞–ª–∏–π
```

**–ò–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞:**
```yaml
–ù–æ–≤—ã–µ —Ç–∞–±–ª–∏—Ü—ã:
  - news_sentiment:
      - article_id
      - sentiment_score: float (-1 to 1)
      - confidence: float (0 to 1)
      - computed_at: timestamp

–ò–∑–º–µ–Ω–µ–Ω–∏—è –≤ –∫–æ–¥–µ:
  - src/services/sentiment_analyzer.py  # –Ω–æ–≤—ã–π —Ñ–∞–π–ª
  - src/services/hotness_scorer.py      # –¥–æ–±–∞–≤–∏—Ç—å sentiment_drift –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
  - src/workers/news_processor.py       # –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è
```

### üî® –°–ª–æ–∂–Ω–æ—Å—Ç—å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏:

**–£—Ä–æ–≤–µ–Ω—å:** ‚≠ê‚≠ê‚≠ê (—Å—Ä–µ–¥–Ω—è—è)

**–≠—Ç–∞–ø—ã —Ä–∞–±–æ—Ç—ã:**
1. **–î–µ–Ω—å 1-2:** –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≥–æ—Ç–æ–≤–æ–π sentiment –º–æ–¥–µ–ª–∏
   - –£—Å—Ç–∞–Ω–æ–≤–∫–∞: `pip install dostoevsky`
   - –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ 100 —Å—Ç–∞—Ç—å—è—Ö
   - –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ (precision ~70-80% –û–ö –¥–ª—è MVP)

2. **–î–µ–Ω—å 3:** –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ pipeline
   ```python
   # –í news_processor.py –¥–æ–±–∞–≤–∏—Ç—å:
   async def process_article(article):
       sentiment = sentiment_analyzer.analyze(article['content'])
       article['sentiment'] = sentiment
       # ... existing code
   ```

3. **–î–µ–Ω—å 4:** –í—ã—á–∏—Å–ª–µ–Ω–∏–µ drift
   ```python
   def calculate_sentiment_drift(articles):
       sentiments = sorted(articles, key=lambda x: x['published_at'])
       scores = [a['sentiment'] for a in sentiments]
       
       # –ü—Ä–æ—Å—Ç–∞—è –≤–µ—Ä—Å–∏—è: —Ä–∞–∑–Ω–∏—Ü–∞ –Ω–∞—á–∞–ª–æ-–∫–æ–Ω–µ—Ü
       drift = abs(scores[-1] - scores[0])
       
       # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è: –≤–∑–≤–µ—à–µ–Ω–Ω–∞—è –ø–æ velocity
       drift_weighted = drift * velocity_factor
       return drift_weighted
   ```

4. **–î–µ–Ω—å 5:** –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Ç—é–Ω–∏–Ω–≥ –≤–µ—Å–æ–≤

**–ü–æ–¥–≤–æ–¥–Ω—ã–µ –∫–∞–º–Ω–∏:**
- ‚ö†Ô∏è Sentiment –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ —Ö—É–∂–µ –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö (70-75% vs 85-90% accuracy)
- ‚ö†Ô∏è –§–∏–Ω–∞–Ω—Å–æ–≤—ã–π —Å–ª–µ–Ω–≥ –º–æ–∂–µ—Ç –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å—Å—è ("–±—ã—á–∏–π —Ç—Ä–µ–Ω–¥", "–º–µ–¥–≤–µ–∂–∏–π —Ä—ã–Ω–æ–∫")
- ‚ö†Ô∏è –ù—É–∂–Ω–∞ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞ –ø–æ—Ä–æ–≥–æ–≤ "–∑–Ω–∞—á–∏–º–æ–≥–æ" drift

**–ß—Ç–æ –¥–µ–ª–∞—Ç—å –µ—Å–ª–∏ –º–∞–ª–æ –≤—Ä–µ–º–µ–Ω–∏:**
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –≥–æ—Ç–æ–≤—É—é –º–æ–¥–µ–ª—å `dostoevsky` –±–µ–∑ fine-tuning
- –ü—Ä–æ—Å—Ç–æ–π drift = `sentiment_last - sentiment_first`
- –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è: —Ç–æ–ª—å–∫–æ –≤ `hotness_scorer.py`

---

## **2. Cross-Asset Contagion Score (–ó–∞—Ä–∞–∂–µ–Ω–∏–µ –º–µ–∂–¥—É –∞–∫—Ç–∏–≤–∞–º–∏)**

### üéØ –ü—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏:
*–ï—Å–ª–∏ –ø–∞–¥–∞–µ—Ç –∫—Ä—É–ø–Ω—ã–π –ø–æ—Å—Ç–∞–≤—â–∏–∫ —á–∏–ø–æ–≤, –ø–æ—Å—Ç—Ä–∞–¥–∞—é—Ç –≤—Å–µ –∫–æ–º–ø–∞–Ω–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –¥–µ–ª–∞—é—Ç —Å–º–∞—Ä—Ç—Ñ–æ–Ω—ã –∏ –∫–æ–º–ø—å—é—Ç–µ—Ä—ã. –ú—ã —Å—Ç—Ä–æ–∏–º –∫–∞—Ä—Ç—É "–∫—Ç–æ –Ω–∞ –∫–æ–≥–æ –≤–ª–∏—è–µ—Ç" –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Ö–æ–¥–∏–º, –∫–∞–∫–∏–µ –¥—Ä—É–≥–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ –º–æ–≥—É—Ç –ø–æ—Å—Ç—Ä–∞–¥–∞—Ç—å –æ—Ç –Ω–æ–≤–æ—Å—Ç–∏.*

### üì¶ –ß—Ç–æ –Ω—É–∂–Ω–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å:

**–î–∞–Ω–Ω—ã–µ:**
```python
# 1. –ì—Ä–∞—Ñ —Å–≤—è–∑–µ–π –∫–æ–º–ø–∞–Ω–∏–π
company_relationships = {
    "supplier_of": [...],      # Tesla –ø–æ—Å—Ç–∞–≤—â–∏–∫ –¥–ª—è X, Y, Z
    "competitor_of": [...],    # Tesla –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç BMW, Toyota
    "same_sector": [...],      # Tesla –≤ –∞–≤—Ç–æ—Å–µ–∫—Ç–æ—Ä–µ —Å Ford
    "customer_of": [...]       # Tesla –∫–ª–∏–µ–Ω—Ç TSMC (—á–∏–ø—ã)
}

# –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö:
- Bloomberg Supply Chain –¥–∞–Ω–Ω—ã–µ (–ø–ª–∞—Ç–Ω–æ, ~$1000/month)
- –ò–õ–ò Wikipedia/Wikidata (–±–µ—Å–ø–ª–∞—Ç–Ω–æ, –Ω–æ –º–µ–Ω–µ–µ —Ç–æ—á–Ω–æ)
- –ò–õ–ò –ø–∞—Ä—Å–∏–Ω–≥ –≥–æ–¥–æ–≤—ã—Ö –æ—Ç—á–µ—Ç–æ–≤ (SEC 10-K, —Ä–∞–∑–¥–µ–ª "Principal Suppliers")
```

**–ú–æ–¥–µ–ª–∏:**
```python
# Graph algorithms (–±–µ–∑ ML)
1. PageRank –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –∫–æ–º–ø–∞–Ω–∏–π
2. Shortest Path –¥–ª—è —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –º–µ–∂–¥—É –∫–æ–º–ø–∞–Ω–∏—è–º–∏
3. Community Detection –¥–ª—è –æ—Ç—Ä–∞—Å–ª–µ–≤—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤

# –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞: networkx (—É–∂–µ –ø–æ–ø—É–ª—è—Ä–Ω–∞—è)
pip install networkx
```

**–ò–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞:**
```yaml
–ù–æ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:
  - data/company_graph.json          # –≥—Ä–∞—Ñ —Å–≤—è–∑–µ–π
  - src/services/graph_analyzer.py   # –∞–Ω–∞–ª–∏–∑ –≥—Ä–∞—Ñ–∞
  - src/db/models.py:
      - CompanyRelationship(from_id, to_id, type, strength)

–ò–∑–º–µ–Ω–µ–Ω–∏—è:
  - src/services/hotness_scorer.py   # –¥–æ–±–∞–≤–∏—Ç—å contagion_score
```

### üî® –°–ª–æ–∂–Ω–æ—Å—Ç—å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏:

**–£—Ä–æ–≤–µ–Ω—å:** ‚≠ê‚≠ê‚≠ê‚≠ê (–≤—ã—Å–æ–∫–∞—è)

**–≠—Ç–∞–ø—ã —Ä–∞–±–æ—Ç—ã:**

1. **–î–µ–Ω—å 1-3:** –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –æ —Å–≤—è–∑—è—Ö
   ```python
   # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è MVP:
   # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç—Ä–∞—Å–ª–µ–≤—É—é –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç—å
   sectors = {
       "Automotive": ["Tesla", "Ford", "BMW", "Toyota"],
       "Tech": ["Apple", "Microsoft", "Google"],
       # ...
   }
   
   # –í—Å–µ –≤ –æ–¥–Ω–æ–º —Å–µ–∫—Ç–æ—Ä–µ = —Å–≤—è–∑–∞–Ω—ã
   ```

2. **–î–µ–Ω—å 4-5:** –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞
   ```python
   import networkx as nx
   
   G = nx.Graph()
   
   # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–º–ø–∞–Ω–∏–∏ –∫–∞–∫ —É–∑–ª—ã
   for company in companies:
       G.add_node(company, market_cap=..., sector=...)
   
   # –î–æ–±–∞–≤–ª—è–µ–º —Å–≤—è–∑–∏ –∫–∞–∫ —Ä–µ–±—Ä–∞
   for rel in relationships:
       G.add_edge(rel['from'], rel['to'], 
                  type=rel['type'],
                  weight=rel['strength'])
   
   # –í—ã—á–∏—Å–ª—è–µ–º PageRank
   centrality = nx.pagerank(G)
   ```

3. **–î–µ–Ω—å 6-7:** –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ —Å–∫–æ—Ä–∏–Ω–≥
   ```python
   def calculate_contagion(affected_company, graph):
       # –ù–∞–π—Ç–∏ —Å–æ—Å–µ–¥–µ–π –≤ –≥—Ä–∞—Ñ–µ
       neighbors = list(graph.neighbors(affected_company))
       
       # –í—ã—á–∏—Å–ª–∏—Ç—å —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ—Å—Ç—å
       centrality_score = pagerank[affected_company]
       
       # –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å –∑–∞—Ç—Ä–æ–Ω—É—Ç—ã—Ö
       affected_count = len(neighbors)
       
       # –§–∏–Ω–∞–ª—å–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞
       contagion = centrality_score * affected_count / 100
       return min(1.0, contagion)
   ```

4. **–î–µ–Ω—å 8-10:** –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –∫–µ–π—Å–∞—Ö
   - –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–æ–±—ã—Ç–∏—è—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–∞–Ω–∫—Ü–∏–∏ –ø—Ä–æ—Ç–∏–≤ –†–§ ‚Üí –≤–ª–∏—è–Ω–∏–µ –Ω–∞ –µ–≤—Ä–æ–ø–µ–π—Å–∫–∏–µ –±–∞–Ω–∫–∏)

**–ü–æ–¥–≤–æ–¥–Ω—ã–µ –∫–∞–º–Ω–∏:**
- ‚ö†Ô∏è **–ì–õ–ê–í–ù–ê–Ø –ü–†–û–ë–õ–ï–ú–ê:** –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ supply chains –¥–æ—Ä–æ–≥–∏–µ –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç
- ‚ö†Ô∏è –ì—Ä–∞—Ñ –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–≥—Ä–æ–º–Ω—ã–º (>10K –∫–æ–º–ø–∞–Ω–∏–π) ‚Üí –Ω—É–∂–Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
- ‚ö†Ô∏è –°–≤—è–∑–∏ –º–µ–Ω—è—é—Ç—Å—è –≤–æ –≤—Ä–µ–º–µ–Ω–∏ (–∫–æ–º–ø–∞–Ω–∏–∏ –º–µ–Ω—è—é—Ç –ø–æ—Å—Ç–∞–≤—â–∏–∫–æ–≤)

**–ß—Ç–æ –¥–µ–ª–∞—Ç—å –µ—Å–ª–∏ –º–∞–ª–æ –≤—Ä–µ–º–µ–Ω–∏:**
```python
# –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è: —Ç–æ–ª—å–∫–æ sector-based
def simple_contagion(company, articles):
    company_sector = get_sector(company)
    
    # –°—á–∏—Ç–∞–µ–º, —Å–∫–æ–ª—å–∫–æ –¥—Ä—É–≥–∏—Ö –∫–æ–º–ø–∞–Ω–∏–π –∏–∑ —Ç–æ–≥–æ –∂–µ —Å–µ–∫—Ç–æ—Ä–∞ —É–ø–æ–º—è–Ω—É—Ç—ã
    same_sector_count = sum(
        1 for art in articles 
        if get_sector(art['entity']) == company_sector
    )
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
    return min(1.0, same_sector_count / 5.0)
```

**–î–ª—è –ø–∏—Ç—á–∞:**
- –ü–æ–∫–∞–∂–∏—Ç–µ –≥—Ä–∞—Ñ –∏–∑ 20-30 –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–º–ø–∞–Ω–∏–π (–≤–∏–∑—É–∞–ª—å–Ω–æ –∫—Ä–∞—Å–∏–≤–æ!)
- –î–µ–º–æ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º –ø—Ä–∏–º–µ—Ä–µ: "–ù–æ–≤–æ—Å—Ç—å –æ Tesla ‚Üí –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—à–ª–∏ 8 —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–π"

---

## **3. Institutional Source Authority Matrix (–ú–∞—Ç—Ä–∏—Ü–∞ –∞–≤—Ç–æ—Ä–∏—Ç–µ—Ç–Ω–æ—Å—Ç–∏)**

### üéØ –ü—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏:
*–ù–µ –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –æ–¥–∏–Ω–∞–∫–æ–≤–æ —Ö–æ—Ä–æ—à–∏ –≤–æ –≤—Å–µ–º. BBC –æ—Ç–ª–∏—á–Ω–æ –¥–ª—è –ø–æ–ª–∏—Ç–∏–∫–∏, –Ω–æ –Ω–µ –ª—É—á—à–∏–π –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤. Forbes —Ö–æ—Ä–æ—à –¥–ª—è –±–∏–∑–Ω–µ—Å–∞, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ Twitter –¥–ª—è breaking news. –ú—ã —Å–æ–∑–¥–∞–µ–º "—Ç–∞–±–µ–ª—å —É—Å–ø–µ–≤–∞–µ–º–æ—Å—Ç–∏" –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –ø–æ —Ä–∞–∑–Ω—ã–º —Ç–µ–º–∞–º.*

### üì¶ –ß—Ç–æ –Ω—É–∂–Ω–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å:

**–î–∞–Ω–Ω—ã–µ:**
```python
# –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
source_performance = {
    "source_id": "bloomberg",
    "accuracy_by_type": {
        "m&a": 0.95,           # 95% –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç—Å—è
        "earnings": 0.92,
        "bankruptcy": 0.88,
        "rumor": 0.45
    },
    "speed_rank": 2,            # 2-–π –ø–æ —Å–∫–æ—Ä–æ—Å—Ç–∏ (1=fastest)
    "correction_rate": 0.02     # 2% —Å—Ç–∞—Ç–µ–π –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É—é—Ç—Å—è –ø–æ–∑–∂–µ
}

# –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö:
1. –í–∞—à–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è (–µ—Å–ª–∏ –µ—Å—Ç—å >3 –º–µ—Å—è—Ü–µ–≤ –¥–∞–Ω–Ω—ã—Ö)
2. –ü—É–±–ª–∏—á–Ω—ã–µ —Ä–µ–ø—É—Ç–∞—Ü–∏–æ–Ω–Ω—ã–µ –±–∞–∑—ã (Media Bias Chart, AllSides)
3. –≠–∫—Å–ø–µ—Ä—Ç–Ω–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞ (–≤—Ä—É—á–Ω—É—é –æ—Ü–µ–Ω–∏—Ç—å —Ç–æ–ø-20 –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤)
```

**–ú–æ–¥–µ–ª–∏:**
```python
# –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ç–∏–ø–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π
- Input: —Ç–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏
- Output: [m&a, earnings, bankruptcy, regulatory, other]

# –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:
1. Keyword-based (–ø—Ä–æ—Å—Ç–æ–π)
2. FastText classifier (—Å—Ä–µ–¥–Ω–∏–π)
3. BERT classifier (—Å–ª–æ–∂–Ω—ã–π)
```

**–ò–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞:**
```yaml
–ù–æ–≤—ã–µ —Ç–∞–±–ª–∏—Ü—ã:
  - source_authority_matrix:
      - source_id
      - news_type
      - accuracy_score
      - sample_count
      - last_updated

–§–∞–π–ª—ã:
  - src/services/news_classifier.py    # –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –Ω–æ–≤–æ—Å—Ç–∏
  - src/services/source_evaluator.py   # –æ—Ü–µ–Ω–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
  - data/source_matrix.json            # –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
```

### üî® –°–ª–æ–∂–Ω–æ—Å—Ç—å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏:

**–£—Ä–æ–≤–µ–Ω—å:** ‚≠ê‚≠ê‚≠ê (—Å—Ä–µ–¥–Ω—è—è)

**–≠—Ç–∞–ø—ã —Ä–∞–±–æ—Ç—ã:**

1. **–î–µ–Ω—å 1-2:** –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π
   ```python
   # Keyword-based –¥–ª—è MVP
   NEWS_TYPE_KEYWORDS = {
       "m&a": ["merger", "acquisition", "—Å–ª–∏—è–Ω–∏–µ", "–ø–æ–≥–ª–æ—â–µ–Ω–∏–µ", "M&A"],
       "earnings": ["earnings", "–ø—Ä–∏–±—ã–ª—å", "revenue", "–≤—ã—Ä—É—á–∫–∞"],
       "bankruptcy": ["bankruptcy", "–±–∞–Ω–∫—Ä–æ—Ç—Å—Ç–≤–æ", "default"],
       "regulatory": ["regulation", "SEC", "—Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ", "–∑–∞–∫–æ–Ω"]
   }
   
   def classify_news_type(title, content):
       text = (title + " " + content).lower()
       scores = {}
       for news_type, keywords in NEWS_TYPE_KEYWORDS.items():
           score = sum(1 for kw in keywords if kw in text)
           scores[news_type] = score
       
       return max(scores, key=scores.get) if max(scores.values()) > 0 else "other"
   ```

2. **–î–µ–Ω—å 3-4:** –°–æ–∑–¥–∞–Ω–∏–µ –Ω–∞—á–∞–ª—å–Ω–æ–π –º–∞—Ç—Ä–∏—Ü—ã
   ```python
   # –≠–∫—Å–ø–µ—Ä—Ç–Ω–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞ —Ç–æ–ø –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
   INITIAL_MATRIX = {
       "bloomberg": {
           "m&a": 0.95,
           "earnings": 0.92,
           "bankruptcy": 0.90,
           "regulatory": 0.88,
           "other": 0.80
       },
       "reuters": {...},
       "rbc.ru": {...},
       # ... —Ç–æ–ø 20 –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
   }
   
   # –î–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: —Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
   DEFAULT_SCORES = {
       "m&a": 0.60,
       "earnings": 0.65,
       # ...
   }
   ```

3. **–î–µ–Ω—å 5-6:** –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ scoring
   ```python
   def calculate_enhanced_credibility(articles):
       scores = []
       for article in articles:
           source = article['source_id']
           news_type = classify_news_type(article['title'], article['content'])
           
           # –ü–æ–ª—É—á–∏—Ç—å score –∏–∑ –º–∞—Ç—Ä–∏—Ü—ã
           authority = get_authority_score(source, news_type)
           scores.append(authority)
       
       return np.mean(scores)
   ```

4. **–î–µ–Ω—å 7:** (–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ
   ```python
   # –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π
   def update_authority_matrix(source, news_type, was_confirmed):
       current_score = matrix[source][news_type]
       
       # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ —Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ
       alpha = 0.1
       new_score = alpha * (1.0 if was_confirmed else 0.0) + (1-alpha) * current_score
       
       matrix[source][news_type] = new_score
   ```

**–ü–æ–¥–≤–æ–¥–Ω—ã–µ –∫–∞–º–Ω–∏:**
- ‚ö†Ô∏è Cold start problem: —á—Ç–æ –¥–µ–ª–∞—Ç—å —Å –Ω–æ–≤—ã–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏?
- ‚ö†Ô∏è –ù—É–∂–µ–Ω –º–µ—Ö–∞–Ω–∏–∑–º –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è (–∫–∞–∫ —É–∑–Ω–∞—Ç—å, —á—Ç–æ –Ω–æ–≤–æ—Å—Ç—å –æ–∫–∞–∑–∞–ª–∞—Å—å –ø—Ä–∞–≤–¥–æ–π?)
- ‚ö†Ô∏è –ú–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç—å –≤ —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–µ

**–ß—Ç–æ –¥–µ–ª–∞—Ç—å –µ—Å–ª–∏ –º–∞–ª–æ –≤—Ä–µ–º–µ–Ω–∏:**
```python
# –ú–∏–Ω–∏–º—É–º: —Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–æ–ø-10 –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
# –û—Å—Ç–∞–ª—å–Ω—ã–µ –ø–æ–ª—É—á–∞—é—Ç –±–∞–∑–æ–≤—ã–π reputation_score –∏–∑ –ë–î
def get_authority(source, news_type):
    if source in KNOWN_MATRIX:
        return KNOWN_MATRIX[source].get(news_type, 0.7)
    else:
        # Fallback to basic reputation
        return sources_db.get_reputation(source)
```

---

## **4. Market Reaction Predictor (–ü—Ä–µ–¥–∏–∫—Ç–æ—Ä –≤–ª–∏—è–Ω–∏—è –Ω–∞ —Ä—ã–Ω–æ–∫)**

### üéØ –ü—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏:
*–ö–æ–≥–¥–∞ —Ç—ã –≤–∏–¥–∏—à—å –Ω–æ–≤–æ—Å—Ç—å "Apple –≤—ã–ø—É—Å—Ç–∏–ª–∞ –Ω–æ–≤—ã–π iPhone", —Ö–æ—á–µ—Ç—Å—è –∑–Ω–∞—Ç—å: "–≠—Ç–æ –≤–∞–∂–Ω–æ? –ê–∫—Ü–∏–∏ –≤—ã—Ä–∞—Å—Ç—É—Ç?" –ú—ã –æ–±—É—á–∞–µ–º —Å–∏—Å—Ç–µ–º—É –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Å–∏–ª—å–Ω–æ —Ü–µ–Ω–∞ –∞–∫—Ü–∏–π –º–æ–∂–µ—Ç –∏–∑–º–µ–Ω–∏—Ç—å—Å—è –ø–æ—Å–ª–µ –Ω–æ–≤–æ—Å—Ç–∏ ‚Äî –≤—Ä–æ–¥–µ –ø—Ä–æ–≥–Ω–æ–∑–∞ –ø–æ–≥–æ–¥—ã, –Ω–æ –¥–ª—è –±–∏—Ä–∂–∏.*

### üì¶ –ß—Ç–æ –Ω—É–∂–Ω–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å:

**–î–∞–Ω–Ω—ã–µ:**
```python
# –û–±—É—á–∞—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç (–Ω—É–∂–Ω–æ —Å–æ–±—Ä–∞—Ç—å!)
training_data = {
    "news_features": {
        "text": "Apple announces merger with...",
        "sentiment": -0.3,
        "source_authority": 0.95,
        "entities": ["AAPL"],
        "news_type": "m&a",
        "time_of_day": "09:30",  # –≤–∞–∂–Ω–æ! –¥–æ/–ø–æ—Å–ª–µ –æ—Ç–∫—Ä—ã—Ç–∏—è –±–∏—Ä–∂–∏
        "market_cap": 2.8e12
    },
    "target": {
        "price_change_1h": 0.03,      # +3% –∑–∞ —á–∞—Å
        "price_change_24h": 0.05,     # +5% –∑–∞ –¥–µ–Ω—å
        "volatility_spike": True,     # –±—ã–ª–∞ –ª–∏ —Ä–µ–∑–∫–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
        "volume_spike": 2.5           # –æ–±—ä–µ–º —Ç–æ—Ä–≥–æ–≤ x2.5 –æ—Ç –Ω–æ—Ä–º—ã
    }
}

# –ò—Å—Ç–æ—á–Ω–∏–∫–∏:
1. –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ + —Ü–µ–Ω—ã –∞–∫—Ü–∏–π (Yahoo Finance API - –±–µ—Å–ø–ª–∞—Ç–Ω–æ)
2. –ù—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 1000-5000 –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
3. –í—Ä–µ–º–µ–Ω–Ω–æ–π –ø–µ—Ä–∏–æ–¥: 6-12 –º–µ—Å—è—Ü–µ–≤ –∏—Å—Ç–æ—Ä–∏–∏
```

**–ú–æ–¥–µ–ª–∏:**
```python
# ML –º–æ–¥–µ–ª—å –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
–í–∞—Ä–∏–∞–Ω—Ç—ã:
1. LightGBM (—Ä–µ–∫–æ–º–µ–Ω–¥—É—é –¥–ª—è MVP)
   - –ë—ã—Å—Ç—Ä—ã–π, —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ CPU
   - –•–æ—Ä–æ—à–æ —Å —Ç–∞–±–ª–∏—á–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
   - 2-3 –¥–Ω—è –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ

2. XGBoost
   - –ß—É—Ç—å —Ç–æ—á–Ω–µ–µ LightGBM
   - –°—Ç–∞–Ω–¥–∞—Ä—Ç –∏–Ω–¥—É—Å—Ç—Ä–∏–∏

3. Neural Network (LSTM)
   - –î–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
   - –°–ª–æ–∂–Ω–µ–µ, —Ç—Ä–µ–±—É–µ—Ç GPU
   - –ù–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é –¥–ª—è –ø–µ—Ä–≤–æ–π –≤–µ—Ä—Å–∏–∏
```

**–ò–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞:**
```yaml
–°–∫—Ä–∏–ø—Ç—ã —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö:
  - scripts/collect_historical_prices.py    # Yahoo Finance
  - scripts/match_news_to_prices.py         # —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –∏ —Ü–µ–Ω
  - scripts/train_market_predictor.py       # –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏

–ù–æ–≤—ã–µ —Ñ–∞–π–ª—ã:
  - src/services/market_predictor.py        # inference
  - models/market_impact_lgbm.pkl           # —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
  - data/market_impact_features.json        # feature engineering

–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:
  - yfinance  # API –¥–ª—è —Ü–µ–Ω
  - lightgbm
  - scikit-learn
```

### üî® –°–ª–æ–∂–Ω–æ—Å—Ç—å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏:

**–£—Ä–æ–≤–µ–Ω—å:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (–æ—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è - —Å–∞–º–∞—è —Å–ª–æ–∂–Ω–∞—è!)

**–≠—Ç–∞–ø—ã —Ä–∞–±–æ—Ç—ã:**

1. **–ù–µ–¥–µ–ª—è 1: –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö**
   ```python
   import yfinance as yf
   
   # 1. –°–∫–∞—á–∞—Ç—å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Ü–µ–Ω—ã
   tickers = ["AAPL", "TSLA", "GAZP.ME", ...]  # —Ç–æ–ø 50-100 –∫–æ–º–ø–∞–Ω–∏–π
   
   for ticker in tickers:
       data = yf.download(ticker, start="2024-01-01", end="2024-12-31")
       # data —Å–æ–¥–µ—Ä–∂–∏—Ç: Open, High, Low, Close, Volume
       save_to_database(ticker, data)
   
   # 2. –°–æ–µ–¥–∏–Ω–∏—Ç—å —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏
   for news in historical_news:
       entity = news['entities'][0]  # –æ—Å–Ω–æ–≤–Ω–∞—è –∫–æ–º–ø–∞–Ω–∏—è
       ticker = entity_to_ticker(entity)  # "Apple" -> "AAPL"
       
       # –ù–∞–π—Ç–∏ —Ü–µ–Ω—É –¥–æ –∏ –ø–æ—Å–ª–µ –Ω–æ–≤–æ—Å—Ç–∏
       price_before = get_price(ticker, news['published_at'])
       price_after_1h = get_price(ticker, news['published_at'] + 1hour)
       price_after_24h = get_price(ticker, news['published_at'] + 24hours)
       
       # –í—ã—á–∏—Å–ª–∏—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏–µ
       change_1h = (price_after_1h - price_before) / price_before
       
       # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–∞–∫ –æ–±—É—á–∞—é—â–∏–π –ø—Ä–∏–º–µ—Ä
       training_examples.append({
           "features": extract_features(news),
           "target": change_1h
       })
   ```

2. **–ù–µ–¥–µ–ª—è 2: Feature Engineering**
   ```python
   def extract_features(news):
       return {
           # –¢–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∏—á–∏
           "sentiment": news['sentiment'],
           "sentiment_magnitude": abs(news['sentiment']),
           "title_length": len(news['title']),
           "content_length": len(news['content']),
           
           # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ
           "news_type": news['news_type'],  # one-hot encoding
           "source_tier": get_source_tier(news['source']),  # 1,2,3
           
           # –í—Ä–µ–º–µ–Ω–Ω—ã–µ
           "hour_of_day": news['published_at'].hour,
           "is_trading_hours": is_market_open(news['published_at']),
           "day_of_week": news['published_at'].weekday(),
           
           # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ
           "market_cap": get_market_cap(news['entities'][0]),
           "recent_volatility": get_historical_volatility(ticker, 30days),
           "sector": get_sector(news['entities'][0]),
           
           # –ú–µ—Ç—Ä–∏–∫–∏ –Ω–æ–≤–æ—Å—Ç–∏
           "hotness_base": news['hotness_score'],
           "num_sources": len(news['cluster']),
           "velocity": news['velocity']
       }
   ```

3. **–ù–µ–¥–µ–ª—è 3: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏**
   ```python
   import lightgbm as lgb
   from sklearn.model_selection import train_test_split
   
   # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
   X = pd.DataFrame([ex['features'] for ex in training_examples])
   y = np.array([ex['target'] for ex in training_examples])
   
   # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
   categorical_features = ['news_type', 'source_tier', 'sector']
   
   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
   
   # –û–±—É—á–µ–Ω–∏–µ
   params = {
       'objective': 'regression',
       'metric': 'rmse',
       'num_leaves': 31,
       'learning_rate': 0.05,
       'feature_fraction': 0.9
   }
   
   model = lgb.train(
       params,
       lgb.Dataset(X_train, y_train, categorical_feature=categorical_features),
       num_boost_round=1000,
       valid_sets=[lgb.Dataset(X_test, y_test)]
   )
   
   # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å
   model.save_model('models/market_impact_lgbm.pkl')
   ```

4. **–ù–µ–¥–µ–ª—è 4: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –∏ –æ—Ü–µ–Ω–∫–∞**
   ```python
   # –í hotness_scorer.py
   def calculate_market_adjusted_hotness(news, base_hotness):
       features = extract_features(news)
       
       # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
       predicted_impact = market_predictor.predict(features)
       
       # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è
       if abs(predicted_impact) > 0.02:  # >2% –¥–≤–∏–∂–µ–Ω–∏–µ
           impact_multiplier = 1.0 + abs(predicted_impact) * 5  # —É—Å–∏–ª–µ–Ω–∏–µ
       else:
           impact_multiplier = 1.0
       
       return base_hotness * impact_multiplier
   ```

**–ü–æ–¥–≤–æ–¥–Ω—ã–µ –∫–∞–º–Ω–∏:**
- ‚ö†Ô∏è **–û–ì–†–û–ú–ù–ê–Ø –ü–†–û–ë–õ–ï–ú–ê:** –ü—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω–∞—è —Å–≤—è–∑—å vs –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è
  - –ù–µ –∫–∞–∂–¥–æ–µ –¥–≤–∏–∂–µ–Ω–∏–µ —Ü–µ–Ω—ã –≤—ã–∑–≤–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç—è–º–∏!
  - –ú–æ–∂–µ—Ç –±—ã—Ç—å –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å: —Ü–µ–Ω–∞ –¥–≤–∏–≥–∞–µ—Ç—Å—è ‚Üí –ø–æ—è–≤–ª—è—é—Ç—Å—è –Ω–æ–≤–æ—Å—Ç–∏
- ‚ö†Ô∏è –ù—É–∂–Ω—ã –¥–∞–Ω–Ω—ã–µ –º–∏–Ω–∏–º—É–º –∑–∞ 6 –º–µ—Å—è—Ü–µ–≤ (–ª—É—á—à–µ –≥–æ–¥)
- ‚ö†Ô∏è –ö–∞—á–µ—Å—Ç–≤–æ —Å–∏–ª—å–Ω–æ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∫–∞—á–µ—Å—Ç–≤–∞ entity extraction
- ‚ö†Ô∏è –†–∞–∑–Ω—ã–µ —Ä—ã–Ω–∫–∏ –≤–µ–¥—É—Ç —Å–µ–±—è –ø–æ-—Ä–∞–∑–Ω–æ–º—É (US vs RU)
- ‚ö†Ô∏è Overfitting ‚Äî –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç "–∑–∞–ø–æ–º–Ω–∏—Ç—å" –ø—Ä–æ—à–ª–æ–µ, –Ω–æ –Ω–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –±—É–¥—É—â–µ–µ

**–ú–µ—Ç—Ä–∏–∫–∏ —É—Å–ø–µ—Ö–∞:**
```python
# –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
from sklearn.metrics import mean_squared_error, r2_score

y_pred = model.predict(X_test)

rmse = mean_squared_error(y_test, y_pred, squared=False)
r2 = r2_score(y_test, y_pred)

print(f"RMSE: {rmse:.4f}")  # –¶–µ–ª—å: <0.03 (3% –æ—à–∏–±–∫–∞)
print(f"R¬≤: {r2:.3f}")      # –¶–µ–ª—å: >0.3 (–æ–±—ä—è—Å–Ω—è–µ—Ç 30%+ –≤–∞—Ä–∏–∞—Ü–∏–∏)

# –î–ª—è –ø–∏—Ç—á–∞:
# "–ú–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Ü–µ–Ω–æ–≤–æ–µ –¥–≤–∏–∂–µ–Ω–∏–µ —Å —Ç–æ—á–Ω–æ—Å—Ç—å—é ¬±2.8%"
# "R¬≤ = 0.42 –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –º—ã –æ–±—ä—è—Å–Ω—è–µ–º 42% –¥–≤–∏–∂–µ–Ω–∏–π —Ü–µ–Ω—ã –Ω–æ–≤–æ—Å—Ç—è–º–∏"
```

**–ß—Ç–æ –¥–µ–ª–∞—Ç—å –µ—Å–ª–∏ –º–∞–ª–æ –≤—Ä–µ–º–µ–Ω–∏:**
```python
# –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è: rule-based predictor
def simple_market_impact(news):
    impact = 0.0
    
    # –ü—Ä–∞–≤–∏–ª–∞
    if news['news_type'] == 'm&a':
        impact += 0.05  # M&A –æ–±—ã—á–Ω–æ –¥–∞–µ—Ç 5%
    
    if news['sentiment'] < -0.7:
        impact += 0.03  # —Å–∏–ª—å–Ω—ã–π –Ω–µ–≥–∞—Ç–∏–≤
    
    if news['source_authority'] > 0.9:
        impact *= 1.5  # –∞–≤—Ç–æ—Ä–∏—Ç–µ—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ —É—Å–∏–ª–∏–≤–∞–µ—Ç
    
    return min(0.10, impact)  # cap –Ω–∞ 10%
```

---

## **5. Breaking News Velocity Spike Detection (–î–µ—Ç–µ–∫—Ç–æ—Ä –≤—Å–ø–ª–µ—Å–∫–æ–≤)**

### üéØ –ü—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏:
*–†–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É "–Ω–æ–≤–æ—Å—Ç–∏ –≤—ã—Ö–æ–¥—è—Ç –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ" –∏ "–ë–ê–•! –≤—Å–µ –°–ú–ò –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –ø–∏—à—É—Ç!". –í—Ç–æ—Ä–æ–µ ‚Äî —Å–∏–≥–Ω–∞–ª, —á—Ç–æ —Å–ª—É—á–∏–ª–æ—Å—å —á—Ç–æ-—Ç–æ –û–ß–ï–ù–¨ –≤–∞–∂–Ω–æ–µ –ø—Ä—è–º–æ —Å–µ–π—á–∞—Å. –ö–∞–∫ —Å–µ–π—Å–º–æ–≥—Ä–∞—Ñ –¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö –∑–µ–º–ª–µ—Ç—Ä—è—Å–µ–Ω–∏–π.*

### üì¶ –ß—Ç–æ –Ω—É–∂–Ω–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å:

**–î–∞–Ω–Ω—ã–µ:**
```python
# –ù—É–∂–Ω—ã —Ç–æ–ª—å–∫–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ (—É–∂–µ –µ—Å—Ç—å!)
article_timestamps = [
    "2024-10-04T09:00:00",
    "2024-10-04T09:05:00",
    "2024-10-04T09:08:00",
    # ...
]

# –ù–∏–∫–∞–∫–æ–≥–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Å–±–æ—Ä–∞ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è!
```

**–ú–æ–¥–µ–ª–∏:**
- –ù–∏–∫–∞–∫–∏—Ö ML –º–æ–¥–µ–ª–µ–π!
- –¢–æ–ª—å–∫–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞: –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ, —Å–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ

**–ò–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞:**
```yaml
–ò–∑–º–µ–Ω–µ–Ω–∏—è:
  - src/services/hotness_scorer.py  # –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å _calculate_velocity
  
–ù–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏:
  - calculate_velocity_spike()
  - calculate_acceleration()
```

### üî® –°–ª–æ–∂–Ω–æ—Å—Ç—å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏:

**–£—Ä–æ–≤–µ–Ω—å:** ‚≠ê (–æ—á–µ–Ω—å –Ω–∏–∑–∫–∞—è - —Å–∞–º–∞—è –ø—Ä–æ—Å—Ç–∞—è!)

**–≠—Ç–∞–ø—ã —Ä–∞–±–æ—Ç—ã:**

1. **–î–µ–Ω—å 1: –†–µ–∞–ª–∏–∑–∞—Ü–∏—è**
   ```python
   def calculate_velocity_spike(articles, time_window_hours=24):
       """
       –í—ã—á–∏—Å–ª—è–µ—Ç spike –≤ velocity –ø—É–±–ª–∏–∫–∞—Ü–∏–π
       """
       if len(articles) < 3:
           return 0.0
       
       # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
       sorted_articles = sorted(articles, key=lambda x: x['published_at'])
       
       # –†–∞–∑–±–∏—Ç—å –Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫–Ω–∞ (–∫–∞–∂–¥—ã–µ 10 –º–∏–Ω—É—Ç)
       window_size = timedelta(minutes=10)
       bins = defaultdict(int)
       
       first_time = sorted_articles[0]['published_at']
       for article in sorted_articles:
           time_diff = article['published_at'] - first_time
           bin_index = int(time_diff.total_seconds() / window_size.total_seconds())
           bins[bin_index] += 1
       
       # –ù–∞–π—Ç–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π spike
       counts = list(bins.values())
       if not counts:
           return 0.0
       
       max_spike = max(counts)
       avg_rate = np.mean(counts) if counts else 1
       
       # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π spike
       spike_ratio = max_spike / (avg_rate + 1e-6)
       
       # –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
       spike_score = min(1.0, np.log1p(spike_ratio) / np.log1p(10))
       
       return spike_score
   ```

2. **–î–µ–Ω—å 2: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è**
   ```python
   # –í hotness_scorer.py
   def _calculate_velocity(self, articles, time_window_hours):
       # –°—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ –¥–ª—è average velocity
       base_velocity = len(articles) / time_span
       normalized_velocity = min(1.0, base_velocity / 2.0)
       
       # –ù–û–í–û–ï: –¥–æ–±–∞–≤–∏—Ç—å spike detection
       spike_score = calculate_velocity_spike(articles, time_window_hours)
       
       # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å
       # –ï—Å–ª–∏ spike –≤—ã—Å–æ–∫–∏–π, —É—Å–∏–ª–∏–≤–∞–µ–º velocity
       velocity_with_spike = normalized_velocity * (1 + spike_score * 0.5)
       
       return min(1.0, velocity_with_spike)
   ```

3. **–î–µ–Ω—å 3: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ**
   ```python
   # –¢–µ—Å—Ç –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
   
   # –°—Ü–µ–Ω–∞—Ä–∏–π A: –†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
   articles_steady = create_articles_every_hour(count=24)
   spike_steady = calculate_velocity_spike(articles_steady)
   # –û–∂–∏–¥–∞–µ–º: ~0.0-0.2
   
   # –°—Ü–µ–Ω–∞—Ä–∏–π B: –í–Ω–µ–∑–∞–ø–Ω—ã–π –≤—Å–ø–ª–µ—Å–∫
   articles_spike = [
       *create_articles_at_time("09:00", count=1),
       *create_articles_at_time("10:00", count=15),  # SPIKE!
       *create_articles_at_time("11:00", count=1)
   ]
   spike_breaking = calculate_velocity_spike(articles_spike)
   # –û–∂–∏–¥–∞–µ–º: ~0.7-0.9
   ```

**–ü–æ–¥–≤–æ–¥–Ω—ã–µ –∫–∞–º–Ω–∏:**
- ‚ö†Ô∏è –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ! –≠—Ç–æ —Å–∞–º–∞—è –ø—Ä–æ—Å—Ç–∞—è —Ñ–∏—á–∞
- ‚ö†Ô∏è –ù—É–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –æ–∫–Ω–∞ (10 –º–∏–Ω? 30 –º–∏–Ω?)
- ‚ö†Ô∏è –ú–æ–∂–µ—Ç –¥–∞–≤–∞—Ç—å false positives –µ—Å–ª–∏ –∏—Å—Ç–æ—á–Ω–∏–∫ –ø—É–±–ª–∏–∫—É–µ—Ç –ø–∞–∫–µ—Ç–∞–º–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —É—Ç—Ä–µ–Ω–Ω–∏–π –¥–∞–π–¥–∂–µ—Å—Ç)

**–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –ø–∏—Ç—á–∞:**
```python
import matplotlib.pyplot as plt

# –ì—Ä–∞—Ñ–∏–∫ –ø—É–±–ª–∏–∫–∞—Ü–∏–π –≤–æ –≤—Ä–µ–º–µ–Ω–∏
plt.figure(figsize=(12, 4))

# –ü—Ä–∏–º–µ—Ä 1: –†–∞–≤–Ω–æ–º–µ—Ä–Ω—ã–π –ø–æ—Ç–æ–∫
plt.subplot(1, 2, 1)
plt.hist(steady_timestamps, bins=24, alpha=0.7, color='blue')
plt.title("–û–±—ã—á–Ω–∞—è –Ω–æ–≤–æ—Å—Ç—å\nSpike Score: 0.15")
plt.xlabel("–í—Ä–µ–º—è (—á–∞—Å—ã)")
plt.ylabel("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—É–±–ª–∏–∫–∞—Ü–∏–π")

# –ü—Ä–∏–º–µ—Ä 2: Breaking news
plt.subplot(1, 2, 2)
plt.hist(spike_timestamps, bins=24, alpha=0.7, color='red')
plt.axvline(x=10, color='red', linestyle='--', label='Breaking moment')
plt.title("Breaking News!\nSpike Score: 0.87")
plt.xlabel("–í—Ä–µ–º—è (—á–∞—Å—ã)")
plt.legend()

plt.tight_layout()
plt.savefig('velocity_spike_demo.png')
```

---

## **6. Narrative Coherence & Contradiction Detector**

### üéØ –ü—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏:
*–ò–Ω–æ–≥–¥–∞ —Ä–∞–∑–Ω—ã–µ –°–ú–ò –ø–∏—à—É—Ç –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤—ã–µ –≤–µ—â–∏: –æ–¥–Ω–∏ –≥–æ–≤–æ—Ä—è—Ç "–∫–æ–º–ø–∞–Ω–∏—è –æ—Ç—Ä–∏—Ü–∞–µ—Ç", –¥—Ä—É–≥–∏–µ "–∏–Ω—Å–∞–π–¥–µ—Ä—ã –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç". –ú—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Ö–æ–¥–∏–º —Ç–∞–∫–∏–µ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è –∏ –ø–æ–Ω–∏–∂–∞–µ–º —Ä–µ–π—Ç–∏–Ω–≥ "–º—É—Ç–Ω—ã—Ö" –∏—Å—Ç–æ—Ä–∏–π, –≥–¥–µ –Ω–µ–ø–æ–Ω—è—Ç–Ω–æ –∫–æ–º—É –≤–µ—Ä–∏—Ç—å.*

### üì¶ –ß—Ç–æ –Ω—É–∂–Ω–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å:

**–î–∞–Ω–Ω—ã–µ:**
```python
# –ü—Ä–∏–º–µ—Ä—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è NLI –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ fine-tuning)
nli_examples = [
    {
        "text1": "–ö–æ–º–ø–∞–Ω–∏—è –ø–æ–¥—Ç–≤–µ—Ä–¥–∏–ª–∞ —Å–ª–∏—è–Ω–∏–µ",
        "text2": "–ü—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å –æ–ø—Ä–æ–≤–µ—Ä–≥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–¥–µ–ª–∫–µ",
        "label": "contradiction"  # –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ
    },
    {
        "text1": "–ê–∫—Ü–∏–∏ –≤—ã—Ä–æ—Å–ª–∏ –Ω–∞ 5%",
        "text2": "–ö–æ—Ç–∏—Ä–æ–≤–∫–∏ –ø–æ–∫–∞–∑–∞–ª–∏ —Ä–æ—Å—Ç",
        "label": "entailment"  # —Å–æ–≥–ª–∞—Å—É–µ—Ç—Å—è
    },
    {
        "text1": "–ó–∞–ø—É—â–µ–Ω –Ω–æ–≤—ã–π –ø—Ä–æ–¥—É–∫—Ç",
        "text2": "–ü–æ–≥–æ–¥–∞ –≤ –ú–æ—Å–∫–≤–µ –¥–æ–∂–¥–ª–∏–≤–∞—è",
        "label": "neutral"  # –Ω–µ —Å–≤—è–∑–∞–Ω–æ
    }
]

# –î–ª—è MVP: –∏—Å–ø–æ–ª—å–∑—É–µ–º –≥–æ—Ç–æ–≤—É—é –º–æ–¥–µ–ª—å, –¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω—É–∂–Ω—ã!
```

**–ú–æ–¥–µ–ª–∏:**
```python
# Natural Language Inference (NLI) –º–æ–¥–µ–ª—å
–í–∞—Ä–∏–∞–Ω—Ç—ã:
1. –ì–æ—Ç–æ–≤–∞—è RuBERT-NLI (–¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ)
   - –£–∂–µ –æ–±—É—á–µ–Ω–∞
   - ~500MB
   - –¢–æ—á–Ω–æ—Å—Ç—å ~75-80%

2. Multilingual NLI (–¥–ª—è –∞–Ω–≥–ª+—Ä—É—Å)
   - xlm-roberta-large-xnli
   - ~1GB
   - –¢–æ—á–Ω–æ—Å—Ç—å ~80-85%

3. API (–µ—Å–ª–∏ –Ω–µ—Ç GPU)
   - OpenAI API
   - Yandex Cloud
   - –°—Ç–æ–∏–º–æ—Å—Ç—å: ~$0.01 –∑–∞ –ø–∞—Ä—É —Ç–µ–∫—Å—Ç–æ–≤
```

**–ò–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞:**
```yaml
–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:
  - transformers  # Hugging Face
  - torch –∏–ª–∏ tensorflow

–§–∞–π–ª—ã:
  - src/services/nli_analyzer.py           # –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π
  - src/services/narrative_coherence.py    # –æ—Ü–µ–Ω–∫–∞ –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏
  - models/rubert_nli/                     # –º–æ–¥–µ–ª—å (–µ—Å–ª–∏ –ª–æ–∫–∞–ª—å–Ω–æ)
```

### üî® –°–ª–æ–∂–Ω–æ—Å—Ç—å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏:

**–£—Ä–æ–≤–µ–Ω—å:** ‚≠ê‚≠ê‚≠ê‚≠ê (–≤—ã—Å–æ–∫–∞—è)

**–≠—Ç–∞–ø—ã —Ä–∞–±–æ—Ç—ã:**

1. **–î–µ–Ω—å 1-2: –ó–∞–≥—Ä—É–∑–∫–∞ NLI –º–æ–¥–µ–ª–∏**
   ```python
   from transformers import pipeline
   
   # –í–∞—Ä–∏–∞–Ω—Ç A: –ì–æ—Ç–æ–≤–∞—è –º–æ–¥–µ–ª—å
   nli_model = pipeline(
       "text-classification",
       model="cointegrated/rubert-base-cased-nli-threeway",
       device=0  # GPU, –∏–ª–∏ -1 –¥–ª—è CPU
   )
   
   # –¢–µ—Å—Ç
   result = nli_model({
       "text": "–ö–æ–º–ø–∞–Ω–∏—è –ø–æ–¥—Ç–≤–µ—Ä–¥–∏–ª–∞ —Å–¥–µ–ª–∫—É",
       "text_pair": "–ü—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å –æ–ø—Ä–æ–≤–µ—Ä–≥ —Å–ª–∏—è–Ω–∏–µ"
   })
   # Output: {'label': 'contradiction', 'score': 0.89}
   ```

2. **–î–µ–Ω—å 3-4: –ü–æ–ø–∞—Ä–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–µ–π**
   ```python
   def find_contradictions(articles):
       """
       –ù–∞—Ö–æ–¥–∏—Ç –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∞—â–∏–µ –ø–∞—Ä—ã —Å—Ç–∞—Ç–µ–π –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
       """
       contradictions = []
       
       # –ò–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –∏–∑ –∫–∞–∂–¥–æ–π —Å—Ç–∞—Ç—å–∏
       # (–º–æ–∂–Ω–æ —É–ø—Ä–æ—Å—Ç–∏—Ç—å: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å title –∏–ª–∏ –ø–µ—Ä–≤—ã–π –∞–±–∑–∞—Ü)
       statements = [extract_key_statement(art) for art in articles]
       
       # –ü–æ–ø–∞—Ä–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ O(n¬≤) - –º–æ–∂–µ—Ç –±—ã—Ç—å –º–µ–¥–ª–µ–Ω–Ω–æ!
       for i in range(len(statements)):
           for j in range(i+1, len(statements)):
               result = nli_model({
                   "text": statements[i],
                   "text_pair": statements[j]
               })
               
               if result['label'] == 'contradiction' and result['score'] > 0.75:
                   contradictions.append({
                       "article1": articles[i],
                       "article2": articles[j],
                       "confidence": result['score']
                   })
       
       return contradictions
   ```

3. **–î–µ–Ω—å 5-6: –í—ã—á–∏—Å–ª–µ–Ω–∏–µ coherence score**
   ```python
   def calculate_narrative_coherence(articles):
       """
       –û—Ü–µ–Ω–∫–∞ —Å–≤—è–∑–Ω–æ—Å—Ç–∏ –Ω–∞—Ä—Ä–∞—Ç–∏–≤–∞
       """
       if len(articles) < 2:
           return 1.0  # –û–¥–Ω–∞ —Å—Ç–∞—Ç—å—è - –Ω–µ—Ç –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π
       
       # 1. –ù–∞–π—Ç–∏ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è
       contradictions = find_contradictions(articles)
       contradiction_ratio = len(contradictions) / (len(articles) * (len(articles)-1) / 2)
       
       # 2. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π
       confirmation_pattern = detect_confirmation_chain(articles)
       # –ò–¥–µ–∞–ª—å–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω: "rumor" -> "industry_report" -> "official_release"
       
       # 3. –ò—Ç–æ–≥–æ–≤—ã–π score
       coherence = 1.0 - contradiction_ratio * 0.5  # –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è —Å–Ω–∏–∂–∞—é—Ç
       coherence *= confirmation_pattern_bonus      # —Ö–æ—Ä–æ—à–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω –ø–æ–≤—ã—à–∞–µ—Ç
       
       return max(0.0, min(1.0, coherence))
   ```

4. **–î–µ–Ω—å 7-8: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ scoring**
   ```python
   # –í hotness_scorer.py
   def calculate_hotness(self, articles, ...):
       # ... existing components ...
       
       # –ù–û–í–û–ï: narrative coherence
       coherence = calculate_narrative_coherence(articles)
       
       # –ü—Ä–∏–º–µ–Ω–∏—Ç—å –∫–∞–∫ –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä
       if coherence < 0.5:
           # –ù–∏–∑–∫–∞—è coherence = —Å–Ω–∏–∂–∞–µ–º –¥–æ–≤–µ—Ä–∏–µ
           credibility *= 0.7
       elif coherence > 0.8:
           # –í—ã—Å–æ–∫–∞—è coherence = —É—Å–∏–ª–∏–≤–∞–µ–º
           credibility *= 1.2
       
       # ... rest of calculation ...
   ```

**–ü–æ–¥–≤–æ–¥–Ω—ã–µ –∫–∞–º–Ω–∏:**
- ‚ö†Ô∏è **–ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–¨:** NLI –º–æ–¥–µ–ª–∏ –º–µ–¥–ª–µ–Ω–Ω—ã–µ (0.1-0.5 —Å–µ–∫ –Ω–∞ –ø–∞—Ä—É)
  - –î–ª—è 10 —Å—Ç–∞—Ç–µ–π = 45 –ø–∞—Ä = 4-20 —Å–µ–∫—É–Ω–¥!
  - –†–µ—à–µ–Ω–∏–µ: –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ, –±–∞—Ç—á–∏–Ω–≥, GPU
- ‚ö†Ô∏è –ú–æ–¥–µ–ª–∏ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ –º–µ–Ω–µ–µ —Ç–æ—á–Ω—ã–µ —á–µ–º –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ
- ‚ö†Ô∏è –°–ª–æ–∂–Ω–æ –∏–∑–≤–ª–µ—á—å "–∫–ª—é—á–µ–≤–æ–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ" –∏–∑ —Å—Ç–∞—Ç—å–∏

**–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:**
```python
# 1. –°—Ä–∞–≤–Ω–∏–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –∑–∞–≥–æ–ª–æ–≤–∫–∏ (–±—ã—Å—Ç—Ä–æ –Ω–æ –º–µ–Ω–µ–µ —Ç–æ—á–Ω–æ)
def quick_contradiction_check(articles):
    titles = [art['title'] for art in articles]
    # ...

# 2. –û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –ø–æ–ø–∞—Ä–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
def smart_contradiction_check(articles, max_pairs=20):
    # –°—Ä–∞–≤–Ω–∏—Ç—å —Ç–æ–ª—å–∫–æ —Ç–æ–ø-N –ø–æ authority –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    top_articles = sorted(articles, key=lambda x: x['authority'])[:5]
    # ...

# 3. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å batch inference
results = nli_model([
    {"text": t1, "text_pair": t2} 
    for t1, t2 in pairs
], batch_size=8)
```

**–î–ª—è –ø–∏—Ç—á–∞:**
```
–°–ª–∞–π–¥: "–î–µ—Ç–µ–∫—Ç–æ—Ä –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π"

[–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è: 2 –∫–æ–ª–æ–Ω–∫–∏]

–õ–µ–≤–∞—è: "Source A (Bloomberg): Company confirms merger"
–ü—Ä–∞–≤–∞—è: "Source B (Twitter): CEO denies any talks"

[–ö—Ä–∞—Å–Ω–∞—è –ª–∏–Ω–∏—è –º–µ–∂–¥—É –Ω–∏–º–∏]
Contradiction Detected! ‚ö†Ô∏è
Confidence: 87%

Action: Lowered credibility score by 30%
‚Üí Story flagged as "developing" (—Ç—Ä–µ–±—É–µ—Ç –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç–∏)
```

---

## **7-10: –ë–æ–ª–µ–µ –ü—Ä–æ—Å—Ç—ã–µ –§–∏—á–∏ (–ö—Ä–∞—Ç–∫–æ)**

### **7. Entity Network Centrality**

**–ü—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏:** *–ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º "–≤–∞–∂–Ω–æ—Å—Ç—å" –∫–∞–∂–¥–æ–π –∫–æ–º–ø–∞–Ω–∏–∏. Apple –≤–∞–∂–Ω–µ–µ –º–µ—Å—Ç–Ω–æ–≥–æ —Å—Ç–∞—Ä—Ç–∞–ø–∞.*

**–°–ª–æ–∂–Ω–æ—Å—Ç—å:** ‚≠ê‚≠ê‚≠ê  
**–í—Ä–µ–º—è:** 3-5 –¥–Ω–µ–π  
**–ù—É–∂–Ω–æ:**
- –ì—Ä–∞—Ñ –∏–∑ –ø.2 (Cross-Asset Contagion) ‚Äî –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º!
- –ü—Ä–æ—Å—Ç–æ –¥–æ–±–∞–≤–∏—Ç—å: `centrality_weight` –∫ breadth –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—É

```python
def enhanced_breadth(articles):
    entities = extract_entities(articles)
    
    # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å
    weighted_count = sum(
        centrality_scores.get(entity, 0.1) 
        for entity in entities
    )
    
    return min(1.0, weighted_count / 3.0)
```

---

### **8. Temporal Decay Adaptation**

**–ü—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏:** *M&A –≤–∞–∂–Ω–æ –Ω–µ–¥–µ–ª—è–º–∏, earnings –≤–∞–∂–Ω–æ —á–∞—Å–∞–º–∏.*

**–°–ª–æ–∂–Ω–æ—Å—Ç—å:** ‚≠ê  
**–í—Ä–µ–º—è:** 1 –¥–µ–Ω—å  
**–ù—É–∂–Ω–æ:** –¢–æ–ª—å–∫–æ –ø—Ä–∞–≤–∏–ª–∞ decay rates

```python
DECAY_RATES = {
    "m&a": 0.05,           # 5% –∑–∞—Ç—É—Ö–∞–Ω–∏–µ –≤ –¥–µ–Ω—å
    "earnings": 0.5,       # 50% –∑–∞—Ç—É—Ö–∞–Ω–∏–µ –≤ –¥–µ–Ω—å
    "bankruptcy": 0.1,
    "default": 0.2
}

def apply_time_decay(hotness, news_type, age_hours):
    rate = DECAY_RATES.get(news_type, 0.2)
    decay_factor = math.exp(-rate * age_hours / 24)
    return hotness * decay_factor
```

---

### **9. Multi-Language Consensus**

**–ü—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏:** *–ï—Å–ª–∏ –ø–∏—à—É—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º –ò –∞–Ω–≥–ª–∏–π—Å–∫–æ–º –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ = –≥–ª–æ–±–∞–ª—å–Ω–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å.*

**–°–ª–æ–∂–Ω–æ—Å—Ç—å:** ‚≠ê‚≠ê  
**–í—Ä–µ–º—è:** 2 –¥–Ω—è  
**–ù—É–∂–Ω–æ:**
- Language detection (langdetect library)
- –ü–æ–¥—Å—á–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤

```python
from langdetect import detect

def calculate_language_diversity(articles):
    languages = set()
    for art in articles:
        try:
            lang = detect(art['title'])
            languages.add(lang)
        except:
            pass
    
    # –ë–æ–Ω—É—Å –∑–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ
    diversity_bonus = len(languages) / 5.0  # 5+ —è–∑—ã–∫–æ–≤ = max
    return min(1.0, diversity_bonus)
```

---

### **10. Controversy Detector (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)**

**–ü—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏:** *–ò—â–µ–º —Å–ª–æ–≤–∞ "–æ—Ç—Ä–∏—Ü–∞–µ—Ç", "–æ–ø—Ä–æ–≤–µ—Ä–≥–∞–µ—Ç" —Ä—è–¥–æ–º —Å "–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç".*

**–°–ª–æ–∂–Ω–æ—Å—Ç—å:** ‚≠ê (–±–µ–∑ NLI –º–æ–¥–µ–ª–∏)  
**–í—Ä–µ–º—è:** 1 –¥–µ–Ω—å  
**Keyword-based –≤–∞—Ä–∏–∞–Ω—Ç:**

```python
CONTROVERSY_KEYWORDS = {
    "deny": ["denies", "–æ—Ç—Ä–∏—Ü–∞–µ—Ç", "–æ–ø—Ä–æ–≤–µ—Ä–≥–∞–µ—Ç", "refutes"],
    "confirm": ["confirms", "–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç", "announces", "–æ–±—ä—è–≤–ª—è–µ—Ç"]
}

def detect_controversy_simple(articles):
    texts = [art['title'] + " " + art['content'] for art in articles]
    combined = " ".join(texts).lower()
    
    has_denial = any(kw in combined for kw in CONTROVERSY_KEYWORDS["deny"])
    has_confirmation = any(kw in combined for kw in CONTROVERSY_KEYWORDS["confirm"])
    
    if has_denial and has_confirmation:
        return 0.8  # –≤—ã—Å–æ–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è
    return 0.0
```

---

## üéØ –§–ò–ù–ê–õ–¨–ù–´–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò

### **–î–ª—è MVP –Ω–∞ –Ω–µ–¥–µ–ª—é:**
1. ‚úÖ Breaking News Spike (1 –¥–µ–Ω—å) ‚Äî –õ–ï–ì–ö–û
2. ‚úÖ Sentiment Drift (3 –¥–Ω—è) ‚Äî –°–†–ï–î–ù–ï
3. ‚úÖ Simple Controversy (1 –¥–µ–Ω—å) ‚Äî –õ–ï–ì–ö–û
4. ‚úÖ Multi-Language (1 –¥–µ–Ω—å) ‚Äî –õ–ï–ì–ö–û

**Total: ~6 –¥–Ω–µ–π + 1 –¥–µ–Ω—å –Ω–∞ –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—é**

### **–î–ª—è –≤–ø–µ—á–∞—Ç–ª—è—é—â–µ–≥–æ –ø–∏—Ç—á–∞:**
1. ‚úÖ Market Reaction Predictor (2 –Ω–µ–¥–µ–ª–∏) ‚Äî WOW!
2. ‚úÖ Cross-Asset Contagion —Å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π –≥—Ä–∞—Ñ–∞ (1.5 –Ω–µ–¥–µ–ª–∏) ‚Äî WOW!
3. ‚úÖ Sentiment Drift (3 –¥–Ω—è)

**Total: ~4 –Ω–µ–¥–µ–ª–∏**

### **–ß—Ç–æ —Ç–æ—á–Ω–æ –ø–æ–∫–∞–∑–∞—Ç—å –∂—é—Ä–∏:**
- **Gra—Ñ –∫–æ–º–ø–∞–Ω–∏–π** (–¥–∞–∂–µ –µ—Å–ª–∏ —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π) ‚Äî –≤–∏–∑—É–∞–ª—å–Ω–æ –∫—Ä–∞—Å–∏–≤–æ
- **Prediction accuracy –º–µ—Ç—Ä–∏–∫–∏** ‚Äî —Ü–∏—Ñ—Ä—ã —É–±–µ–∂–¥–∞—é—Ç
- **Side-by-side –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π** ‚Äî –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –¥–µ—Ç–µ–∫—Ü–∏—é –¥–µ–∑–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏

–ö–∞–∫–∏–µ –∏–∑ —Ñ–∏—á —Ö–æ—Ç–∏—Ç–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å? –ú–æ–≥—É –Ω–∞—á–∞—Ç—å —Å –∫–æ–¥–∞! üöÄ